{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 请注意以下路径无需修改\n",
    "# 框架相关路径\n",
    "global_root_path = rf'/home/lwyxyz'\n",
    "minute_fea_path = rf'{global_root_path}/Stock60sBaseDataAll/Feather'\n",
    "minute_mmap_path = rf'{global_root_path}/Stock60sBaseDataAll/Mmap'\n",
    "support_data_path = rf'{global_root_path}/Stock60sConfig/support_data'\n",
    "# 日频原始数据\n",
    "data79_root_path = rf'{global_root_path}/2.79'\n",
    "stock_daily_data_path1 = rf'{data79_root_path}/tonglian_data/ohlc_fea'\n",
    "stock_daily_data_path2 = rf'{data79_root_path}/tonglian_data/support_data'\n",
    "stock_daily_data_path3 = rf'{data79_root_path}/update/短周期依赖数据'\n",
    "# 高频原始数据\n",
    "local_data_path = rf'{global_root_path}/254.35/data/LocalDataLoader/LocalData'\n",
    "trans_data_path = rf'{local_data_path}/StockTransData'\n",
    "order_data_path = rf'{local_data_path}/StockOrderData'\n",
    "lob_data_path = rf'{global_root_path}/hft_database/nas3/sec_lobdata'\n",
    "\n",
    "\n",
    "def get_all_trade_days():\n",
    "    read_file = rf\"{stock_daily_data_path2}/trade_days_dict.pkl\"\n",
    "    all_trade_days = pd.read_pickle(read_file)['trade_days']\n",
    "    all_trade_days = [x.strftime('%Y%m%d') for x in all_trade_days]\n",
    "    all_trade_days.sort()\n",
    "    return all_trade_days\n",
    "\n",
    "\n",
    "def get_trade_days(start_date, end_date):\n",
    "    all_trade_days = get_all_trade_days()\n",
    "    trade_days = [date for date in all_trade_days if start_date <= date <= end_date]\n",
    "    return trade_days\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def dump_pickle(path, data):\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def read_stock_daily_data(field):\n",
    "    read_path = rf\"{local_data_path}/StockDailyData\"\n",
    "    read_file = \"%s/%s.fea\" % (read_path, field)\n",
    "    read_data = pd.read_feather(read_file).set_index(\"date\")\n",
    "    return read_data\n",
    "\n",
    "\n",
    "def read_high_freq_data(date, name):\n",
    "    if name == \"StockLob\":\n",
    "        read_file = rf\"{lob_data_path}/{date}.fea\"\n",
    "    else:\n",
    "        read_file = \"%s/%sData/%s.fea\" % (local_data_path, name, date)\n",
    "    read_data = pd.read_feather(read_file)\n",
    "    return read_data\n",
    "\n",
    "\n",
    "def read_all_support_dict():\n",
    "    support_dict_list = [x for x in os.listdir(support_data_path) if \"_loc_dict\" in x]\n",
    "    all_dict = {}\n",
    "    for x in support_dict_list:\n",
    "        try:\n",
    "            dict_name = x.split(\"trade_\")[1].split(\"_loc_dict\")[0]\n",
    "            all_dict[dict_name] = load_pickle(rf\"{support_data_path}/{x}\")\n",
    "        except:\n",
    "            pass\n",
    "    return all_dict\n",
    "\n",
    "\n",
    "def multiple_data(data_all, proc_num):\n",
    "    step = len(data_all) // proc_num\n",
    "    data_multiple = []\n",
    "    spilt_code_list = [data_all.iloc[i * step][\"code\"] for i in range(proc_num)]\n",
    "    spilt_num_list = list(\n",
    "        data_all.loc[data_all.code.isin(spilt_code_list)].drop_duplicates([\"code\"], keep=\"last\").index)\n",
    "    spilt_num_list[0] = -1\n",
    "    spilt_num_list += [len(data_all)]\n",
    "    for i1, i2 in zip(spilt_num_list[:-1], spilt_num_list[1:]):\n",
    "        data_multiple.append(data_all.loc[i1 + 1:i2])\n",
    "\n",
    "    return data_multiple\n",
    "\n",
    "\n",
    "def get_second_60s(time_10ms):\n",
    "    div = 100000\n",
    "    return round(round(time_10ms // div) * div)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 需要设定：本批字段的名称，计算的起止日期\n",
    "base_data_name = rf'higherclosevolsum'\n",
    "start_date = '20211231'\n",
    "end_date = '20240930'\n",
    "\n",
    "# 计算和保存每日数据\n",
    "# 基础字段保存在minute_fea_path的base_data_name子目录下\n",
    "# minute_fea_path这个路径会映射到当前用户的home目录下的data_share/Stock60sBaseDataAll/Feather下，是同一个路径\n",
    "fea_save_path = rf'{minute_fea_path}/{base_data_name}'\n",
    "os.makedirs(fea_save_path, exist_ok=True)\n",
    "#os.chmod(fea_save_path, 0o755)\n",
    "trade_date_list = get_trade_days(start_date, end_date)\n",
    "\n",
    "all_support_dict = read_all_support_dict()\n",
    "standard_time_list = list(all_support_dict[rf\"time_60s\"].keys())  # 分钟标记\n",
    "standard_time_num = len(standard_time_list)  # 分钟数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对60秒聚合的数据进行进一步处理\n",
    "def proc_second_data(second_data_ori):\n",
    "    second_data = {}\n",
    "    for field in second_data_ori.columns:\n",
    "        df = second_data_ori[field].unstack(0)\n",
    "\n",
    "        # 必须处理的部分：时间轴对齐，保证股票代码为六位\n",
    "        df = df.reindex(standard_time_list)  # 标准化时间轴\n",
    "        df.columns = df.columns.map(lambda x: x[:6])  # 标准化代码\n",
    "\n",
    "        # 自由处理的部分：对价格数据进行填前值，对量和金额填0等等，和字段的计算逻辑相关\n",
    "        # 例如，对价格等序列，可以填前值\n",
    "        # if (field in price_field):\n",
    "        #     df = df.replace(0, np.nan).ffill().fillna(today_stock_pre_close)\n",
    "        # 例如，对量额等序列，可以填0\n",
    "        # elif (field in volume_amount_field) :\n",
    "        #     df = df.fillna(0.0)\n",
    "        # else:\n",
    "        #     pass\n",
    "\n",
    "        df = df.fillna(0.0)\n",
    "\n",
    "        second_data[field] = df.stack()\n",
    "\n",
    "    second_data = pd.concat(second_data, axis=1)\n",
    "    return second_data\n",
    "\n",
    "\n",
    "def format_second_data(second_data):\n",
    "    float_cols = [x for x in second_data.columns if x not in [\"code\", \"second\"]]\n",
    "    # 所有字段按float32保存\n",
    "    second_data[float_cols] = second_data[float_cols].astype('float32')\n",
    "    # 这里需要把列名改为code+second+字段名的顺序，否则后续转存为mmap时会有异常\n",
    "    second_data = second_data[[\"code\", \"second\"] + float_cols]\n",
    "    return second_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_microseconds(time_str):\n",
    "         # 确保是九位数\n",
    "        time_str = str(time_str).zfill(9)\n",
    " \n",
    "         # 计算微秒数\n",
    "        microseconds = (int(time_str[:2])* 3600 + int(time_str[2:4])* 60 + int(time_str[4:6])) * 1000 + int(time_str[6:])\n",
    "   \n",
    "        return microseconds\n",
    "\n",
    "\n",
    "# 获取某一日的基础字段，以feather存储\n",
    "def get_higher_close_fea(date, proc_num,multi=1):\n",
    "    \"\"\"\n",
    "     :param date: 基础字段的日期\n",
    "     :param proc_num: 计算使用的进程数\n",
    "    \"\"\"\n",
    "\n",
    "    s= read_high_freq_data(date, name=\"StockTrans\").loc[:,['code','time','tradePrice','tradeVolume']]\n",
    "     # 逐笔成交字段s\n",
    "    s[\"second\"] = s[\"time\"].map(get_second_60s)\n",
    "    s=s.groupby(['code','second']).apply(lambda group:group[group['tradePrice']>group['tradePrice'].iloc[-1]]['tradeVolume'].sum())\n",
    "    s=s.reset_index(name='higher_close_volsum')\n",
    "    s.set_index(['code','second'],inplace=True)\n",
    "    seconddt= proc_second_data(s)\n",
    "            #输入 输出都有两列索引\n",
    "            #second_data = second_data.reset_index().sort_values([\"code\", \"second\"]).reset_index(drop=True)\n",
    "    seconddt=seconddt.reset_index()\n",
    "    second_data = format_second_data(seconddt)\n",
    "    second_data .to_feather(rf'{fea_save_path}/{date}.fea', compression=\"zstd\")\n",
    "    os.chmod(rf'{fea_save_path}/{date}.fea', 0o755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [75:00:42<00:00, 405.47s/it]   \n"
     ]
    }
   ],
   "source": [
    "for date in tqdm(trade_date_list):\n",
    "     get_higher_close_fea(date, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_higher_close_fea('20210104', 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
